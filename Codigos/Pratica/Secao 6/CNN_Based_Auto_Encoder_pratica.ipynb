{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa31ca7-e5a1-4221-b866-387105f4f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "    transforms.Lambda(lambda x: x.to(device))\n",
    "])\n",
    "\n",
    "trn_ds = MNIST('', transform=img_transform, train=True, download=True)\n",
    "val_ds = MNIST('', transform=img_transform, train=False, download=True)\n",
    "\n",
    "batch_size = 128\n",
    "trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37293d07-ae95-414a-adeb-2dba7f5c9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_summary in /opt/anaconda3/lib/python3.12/site-packages (1.4.5)\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 64, 3, 3]            --\n",
      "|    └─Conv2d: 2-1                       [-1, 32, 14, 14]          320\n",
      "|    └─ReLU: 2-2                         [-1, 32, 14, 14]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 32, 7, 7]            --\n",
      "|    └─Conv2d: 2-4                       [-1, 64, 4, 4]            18,496\n",
      "|    └─ReLU: 2-5                         [-1, 64, 4, 4]            --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 64, 3, 3]            --\n",
      "├─Sequential: 1-2                        [-1, 1, 28, 28]           --\n",
      "|    └─ConvTranspose2d: 2-7              [-1, 32, 7, 7]            18,464\n",
      "|    └─ReLU: 2-8                         [-1, 32, 7, 7]            --\n",
      "|    └─ConvTranspose2d: 2-9              [-1, 16, 15, 15]          12,816\n",
      "|    └─ReLU: 2-10                        [-1, 16, 15, 15]          --\n",
      "|    └─ConvTranspose2d: 2-11             [-1, 1, 28, 28]           65\n",
      "|    └─Tanh: 2-12                        [-1, 1, 28, 28]           --\n",
      "==========================================================================================\n",
      "Total params: 50,161\n",
      "Trainable params: 50,161\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.23\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.30\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 64, 3, 3]            --\n",
       "|    └─Conv2d: 2-1                       [-1, 32, 14, 14]          320\n",
       "|    └─ReLU: 2-2                         [-1, 32, 14, 14]          --\n",
       "|    └─MaxPool2d: 2-3                    [-1, 32, 7, 7]            --\n",
       "|    └─Conv2d: 2-4                       [-1, 64, 4, 4]            18,496\n",
       "|    └─ReLU: 2-5                         [-1, 64, 4, 4]            --\n",
       "|    └─MaxPool2d: 2-6                    [-1, 64, 3, 3]            --\n",
       "├─Sequential: 1-2                        [-1, 1, 28, 28]           --\n",
       "|    └─ConvTranspose2d: 2-7              [-1, 32, 7, 7]            18,464\n",
       "|    └─ReLU: 2-8                         [-1, 32, 7, 7]            --\n",
       "|    └─ConvTranspose2d: 2-9              [-1, 16, 15, 15]          12,816\n",
       "|    └─ReLU: 2-10                        [-1, 16, 15, 15]          --\n",
       "|    └─ConvTranspose2d: 2-11             [-1, 1, 28, 28]           65\n",
       "|    └─Tanh: 2-12                        [-1, 1, 28, 28]           --\n",
       "==========================================================================================\n",
       "Total params: 50,161\n",
       "Trainable params: 50,161\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.23\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.19\n",
       "Estimated Total Size (MB): 0.30\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "from torchsummary import summary\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 5, stride=2, padding=1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, 2, stride=2, padding=1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = ConvAutoEncoder().to(device)\n",
    "!pip install torch_summary\n",
    "from torchsummary import summary\n",
    "summary(model, torch.zeros(2, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62630014-1ebb-4892-bda1-0bd0fe0fc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input,model,criterion,optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = criterion(output, input)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(input,model,criterion):\n",
    "    model.eval()\n",
    "    output = model(input)\n",
    "    loss = criterion(output, input)\n",
    "    return loss\n",
    "\n",
    "model = ConvAutoEncoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f489cfb-f2d9-4268-b835-9fb68f1d7472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Epoch 1/5 started.<p>Epoch 1/5 completed.<p>Epoch 2/5 started.<p>Epoch 2/5 completed.<p>Epoch 3/5 started.<p>Epoch 3/5 completed.<p>Epoch 4/5 started.<p>Epoch 4/5 completed.<p>Epoch 5/5 started.<p>Epoch 5/5 completed."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "num_epochs = 5\n",
    "log = master_bar(range(num_epochs))\n",
    "\n",
    "for epoch in log:\n",
    "    log.write(f\"Epoch {epoch + 1}/{num_epochs} started.\")\n",
    "    \n",
    "    N = len(trn_dl)\n",
    "    for ix, (data, _) in enumerate(progress_bar(trn_dl, parent=log)):\n",
    "        loss = train_batch(data, model, criterion, optimizer)\n",
    "        log.child.comment = f\"Training Loss: {loss:.4f}\"\n",
    "\n",
    "    N = len(val_dl)\n",
    "    for ix, (data, _) in enumerate(progress_bar(val_dl, parent=log)):\n",
    "        loss = validate_batch(data, model, criterion)\n",
    "        log.child.comment = f\"Validation Loss: {loss:.4f}\"\n",
    "        \n",
    "\n",
    "    log.write(f\"Epoch {epoch + 1}/{num_epochs} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41437200-19f2-466b-8d34-b3cdf4cbcf19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m latent_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(latent_vectors)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Aplicando t-SNE\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m tsne \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m clustered \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(latent_vectors)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Visualização\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "latent_vectors = []\n",
    "classes = []\n",
    "\n",
    "for im, clss in val_dl:\n",
    "    # Passando as imagens pela rede e armazenando os vetores latentes\n",
    "    latent_vectors.append(model.encoder(im).view(im.size(0), -1))\n",
    "    classes.extend(clss)\n",
    "\n",
    "latent_vectors = torch.cat(latent_vectors).cpu().detach().numpy()\n",
    "\n",
    "# Aplicando t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "clustered = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "# Visualização\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "cmap = plt.get_cmap('Spectral', 10)\n",
    "plt.scatter(*zip(*clustered), c=classes, cmap=cmap)\n",
    "plt.colorbar(drawedges=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea429c-c3cd-42b3-b55f-714980645767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
